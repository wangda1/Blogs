文本数据的预处理根据语种而有些不同，如：

- 中文数据的文本预处理过程：分词、去停用词 ...
- 英文数据的文本预处理过程：分词、去停用词、**提取词干** ...

## 1. 中文数据的预处理

1.1 统计词频的几种方法：

## sklearn.feature_extraction.text

使用 `sklearn` 的包有两种统计词频的方法，第一种方法使用 wordcount，第二种方法则使用 TF-IDF方法

- `CountVectorizer`

`CountVectorizer` 统计词频后会将其表示成二维矩阵的形式，（i, j）代表 j 列所代表的的词在 i 文档中的词频，使用方式如下：

```python
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(corpus)            # 输出形式：（i，j） count
feature_name = vectorizer.get_feature_name()    # 输出形式为 词列表
X.toarray()                                     # 输出形式为 矩阵
```

- `TfidfTransformer`

`TfidfTransformer` 可对 `CountVectorizer` 向量化后的数据进行 TF-IDF 的计算，给出 TF-IDF 值，使用方式如下：

```python
transformer = TfidfTransformer()
tfidf = transformer.fit_transform(vectorizer.fit_transform(corpus))
```

- `TfidfVectorizer`

`TfidfVectorizer` = `CountVectorizer` + `TfidfTransformer`， 使用方式为：

```python
tfidf2 = TfidfVectorizer()
re = tfidf2.fit_transform(corpus)
```

## 2. 英文数据的预处理

### 2.1 Sentence Tokenization

```python
text = "Backgammon is one of the oldest known board games. Its history can be traced back nearly 5,000 years to archeological discoveries in the Middle East. It is a two player game where each player has fifteen checkers which move between twenty-four points according to the roll of two dice."
sentences = nltk.sent_tokenize(text)
for sentence in sentences:
    print(sentence)
    print()
```

Output:

Backgammon is one of the oldest known board games.

Its history can be traced back nearly 5,000 years to archeological discoveries in the Middle East.

It is a two player game where each player has fifteen checkers which move between twenty-four points according to the roll of two dice.

### 2.2 Word Tokenization

```python
for sentence in sentences:
    words = nltk.word_tokenize(sentence)
    print(words)
    print()
```

Output:

['Backgammon', 'is', 'one', 'of', 'the', 'oldest', 'known', 'board', 'games', '.']

['Its', 'history', 'can', 'be', 'traced', 'back', 'nearly', '5,000', 'years', 'to', 'archeological', 'discoveries', 'in', 'the', 'Middle', 'East', '.']

['It', 'is', 'a', 'two', 'player', 'game', 'where', 'each', 'player', 'has', 'fifteen', 'checkers', 'which', 'move', 'between', 'twenty-four', 'points', 'according', 'to', 'the', 'roll', 'of', 'two', 'dice', '.']

### 2.3 Text Lemmatization and Stemming

For grammatical reasons, documents can contain different forms of a word such as drive, drives, driving. Also, sometimes we have related words with a similar meaning, such as nation, national, nationality.

**The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form.**

Source: https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html

Examples:

- am, are, is => be
- dog, dogs, dog’s, dogs’ => dog

The result of this mapping applied on a text will be something like that:   
*the boy’s dogs are different sizes => the boy dog be differ size*   
Stemming and lemmatization are special cases of normalization. However, they are different from each other.

> Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes.
> Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma.

Source: https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html

The difference is that a stemmer operates **without knowledge of the context**, and therefore cannot understand the difference between words which have different meaning depending on part of speech. But the stemmers also have some advantages, they are easier to implement and usually run faster. Also, the reduced “accuracy” may not matter for some applications.

Examples:

1. The word “better” has “good” as its lemma. This link is missed by stemming, as it requires a dictionary look-up.
2. The word “play” is the base form for the word “playing”, and hence this is matched in both stemming and lemmatization.
3. The word “meeting” can be either the base form of a noun or a form of a verb (“to meet”) depending on the context; e.g., “in our last meeting” or “We are meeting again tomorrow”. Unlike stemming, lemmatization attempts to select the correct lemma depending on the context.

After we know what’s the difference, let’s see some examples using the NLTK tool.

```python
from nltk.stem import PorterStemmer, WordNetLemmatizer
from nltk.corpus import wordnet

def compare_stemmer_and_lemmatizer(stemmer, lemmatizer, word, pos):
    """
    Print the results of stemmind and lemmitization using the passed stemmer, lemmatizer, word and pos (part of speech)
    """
    print("Stemmer:", stemmer.stem(word))
    print("Lemmatizer:", lemmatizer.lemmatize(word, pos))
    print()

lemmatizer = WordNetLemmatizer()
stemmer = PorterStemmer()
compare_stemmer_and_lemmatizer(stemmer, lemmatizer, word = "seen", pos = wordnet.VERB)
compare_stemmer_and_lemmatizer(stemmer, lemmatizer, word = "drove", pos = wordnet.VERB)
```
Output:  

Stemmer: seen  
Lemmatizer: see

Stemmer: drove  
Lemmatizer: drive  


### 2.4 Stop words

The NLTK tool has a predefined list of stopwords that refers to the most common words. If you use it for your first time, you need to download the stop words using this code: `nltk.download(“stopwords”)`. Once we complete the downloading, we can load the stopwords package from the nltk.corpus and use it to load the stop words.

```python
from nltk.corpus import stopwords
print(stopwords.words("english"))
```

outputs:  
...

```python
stop_words = set(stopwords.words("english"))
sentence = "Backgammon is one of the oldest known board games."

words = nltk.word_tokenize(sentence)
without_stop_words = [word for word in words if not word in stop_words]
print(without_stop_words)
```

outputs:

['Backgammon', 'one', 'oldest', 'known', 'board', 'games', '.']

### 2.5 Bag-of-words

### 2.6 Tools

- `nltk` for pre-processing texts

- `sklearn` for bag-of-words models


## 3. 总结

- 无论采用何种对非结构化文档向量化的 BoW 方法，均是通过 `dictionary` 与 `Sparse Matrix`两者结合的方式来展示，`Sparse Matrix` = `Sparse dictionary index matrix`


## 4. 学习参考

- https://machinelearningmastery.com/clean-text-machine-learning-python/
